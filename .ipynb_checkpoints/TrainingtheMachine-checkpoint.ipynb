{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import io\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Identifier</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>[[[205.]\\n  [205.]\\n  [203.]\\n  [203.]\\n  [205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>[[[205.]\\n  [205.]\\n  [203.]\\n  [203.]\\n  [205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>[[[206.]\\n  [205.]\\n  [205.]\\n  [205.]\\n  [206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>[[[205.]\\n  [206.]\\n  [209.]\\n  [208.]\\n  [206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>[[[206.]\\n  [206.]\\n  [209.]\\n  [207.]\\n  [208...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Identifier                                                  X\n",
       "0           0          A  [[[205.]\\n  [205.]\\n  [203.]\\n  [203.]\\n  [205...\n",
       "1           1          A  [[[205.]\\n  [205.]\\n  [203.]\\n  [203.]\\n  [205...\n",
       "2           2          A  [[[206.]\\n  [205.]\\n  [205.]\\n  [205.]\\n  [206...\n",
       "3           3          A  [[[205.]\\n  [206.]\\n  [209.]\\n  [208.]\\n  [206...\n",
       "4           4          A  [[[206.]\\n  [206.]\\n  [209.]\\n  [207.]\\n  [208..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('numpyz.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Identifier'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "encoded_y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class: A\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: B\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: C\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: D\n",
      "Encoded Label: 3\n",
      "------------\n",
      "Original Class: E\n",
      "Encoded Label: 4\n",
      "------------\n",
      "Original Class: F\n",
      "Encoded Label: 5\n",
      "------------\n",
      "Original Class: G\n",
      "Encoded Label: 6\n",
      "------------\n",
      "Original Class: H\n",
      "Encoded Label: 7\n",
      "------------\n",
      "Original Class: I\n",
      "Encoded Label: 8\n",
      "------------\n",
      "Original Class: J\n",
      "Encoded Label: 9\n",
      "------------\n",
      "Original Class: K\n",
      "Encoded Label: 10\n",
      "------------\n",
      "Original Class: L\n",
      "Encoded Label: 11\n",
      "------------\n",
      "Original Class: M\n",
      "Encoded Label: 12\n",
      "------------\n",
      "Original Class: N\n",
      "Encoded Label: 13\n",
      "------------\n",
      "Original Class: O\n",
      "Encoded Label: 14\n",
      "------------\n",
      "Original Class: P\n",
      "Encoded Label: 15\n",
      "------------\n",
      "Original Class: Q\n",
      "Encoded Label: 16\n",
      "------------\n",
      "Original Class: R\n",
      "Encoded Label: 17\n",
      "------------\n",
      "Original Class: S\n",
      "Encoded Label: 18\n",
      "------------\n",
      "Original Class: T\n",
      "Encoded Label: 19\n",
      "------------\n",
      "Original Class: U\n",
      "Encoded Label: 20\n",
      "------------\n",
      "Original Class: V\n",
      "Encoded Label: 21\n",
      "------------\n",
      "Original Class: W\n",
      "Encoded Label: 22\n",
      "------------\n",
      "Original Class: X\n",
      "Encoded Label: 23\n",
      "------------\n",
      "Original Class: Y\n",
      "Encoded Label: 24\n",
      "------------\n",
      "Original Class: Z\n",
      "Encoded Label: 25\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for label, original_class in zip(encoded_y, y):\n",
    "    print('Original Class: ' + str(original_class))\n",
    "    print('Encoded Label: ' + str(label))\n",
    "    print('-' * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "one_hot_y = to_categorical(encoded_y)\n",
    "one_hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4658, 1, 900) (26, 26)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('linksnp.npy')\n",
    "y = one_hot_y\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8039216 , 0.8039216 , 0.79607844, 0.79607844, 0.8039216 ,\n",
       "        0.8039216 , 0.8039216 , 0.8117647 , 0.8       , 0.8039216 ,\n",
       "        0.79607844, 0.79607844, 0.8039216 , 0.8039216 , 0.8       ,\n",
       "        0.8039216 , 0.7921569 , 0.7921569 , 0.8       , 0.8       ,\n",
       "        0.78039217, 0.7921569 , 0.78431374, 0.79607844, 0.8039216 ,\n",
       "        0.79607844, 0.79607844, 0.8039216 , 0.78431374, 0.79607844,\n",
       "        0.8039216 , 0.79607844, 0.8156863 , 0.8039216 , 0.80784315,\n",
       "        0.8       , 0.8039216 , 0.8039216 , 0.8       , 0.8039216 ,\n",
       "        0.8039216 , 0.79607844, 0.8       , 0.8       , 0.8039216 ,\n",
       "        0.78431374, 0.78039217, 0.80784315, 0.7921569 , 0.78431374,\n",
       "        0.7921569 , 0.78039217, 0.78039217, 0.8       , 0.7882353 ,\n",
       "        0.79607844, 0.8039216 , 0.79607844, 0.79607844, 0.8039216 ,\n",
       "        0.79607844, 0.8039216 , 0.8       , 0.8117647 , 0.80784315,\n",
       "        0.8       , 0.81960785, 0.8039216 , 0.8117647 , 0.80784315,\n",
       "        0.80784315, 0.8039216 , 0.80784315, 0.8039216 , 0.8039216 ,\n",
       "        0.8039216 , 0.8039216 , 0.79607844, 0.78431374, 0.7921569 ,\n",
       "        0.78039217, 0.79607844, 0.7921569 , 0.78431374, 0.7921569 ,\n",
       "        0.7921569 , 0.8039216 , 0.7921569 , 0.78431374, 0.79607844,\n",
       "        0.79607844, 0.79607844, 0.8       , 0.8156863 , 0.8039216 ,\n",
       "        0.8       , 0.83137256, 0.8039216 , 0.8039216 , 0.8039216 ,\n",
       "        0.80784315, 0.8156863 , 0.8117647 , 0.8039216 , 0.80784315,\n",
       "        0.8       , 0.78431374, 0.7921569 , 0.7921569 , 0.78431374,\n",
       "        0.7921569 , 0.78039217, 0.78431374, 0.78431374, 0.78431374,\n",
       "        0.7921569 , 0.77254903, 0.78431374, 0.79607844, 0.78431374,\n",
       "        0.79607844, 0.79607844, 0.8       , 0.7921569 , 0.8039216 ,\n",
       "        0.8117647 , 0.8039216 , 0.8039216 , 0.80784315, 0.80784315,\n",
       "        0.80784315, 0.8039216 , 0.80784315, 0.8039216 , 0.80784315,\n",
       "        0.8352941 , 0.81960785, 0.8       , 0.78039217, 0.7490196 ,\n",
       "        0.6745098 , 0.47843137, 0.7882353 , 0.78431374, 0.7921569 ,\n",
       "        0.78431374, 0.78431374, 0.79607844, 0.78431374, 0.78431374,\n",
       "        0.7921569 , 0.78039217, 0.8       , 0.8       , 0.8039216 ,\n",
       "        0.8       , 0.8235294 , 0.80784315, 0.79607844, 0.8039216 ,\n",
       "        0.8039216 , 0.80784315, 0.8117647 , 0.8235294 , 0.7921569 ,\n",
       "        0.8039216 , 0.7921569 , 0.6666667 , 0.79607844, 0.7372549 ,\n",
       "        0.6901961 , 0.68235296, 0.77254903, 0.78039217, 0.79607844,\n",
       "        0.7882353 , 0.78431374, 0.7921569 , 0.78039217, 0.8039216 ,\n",
       "        0.7921569 , 0.7921569 , 0.79607844, 0.8       , 0.79607844,\n",
       "        0.83137256, 0.79607844, 0.8039216 , 0.8117647 , 0.80784315,\n",
       "        0.79607844, 0.8039216 , 0.7529412 , 0.7921569 , 0.8156863 ,\n",
       "        0.7882353 , 0.7764706 , 0.58431375, 0.7254902 , 0.68235296,\n",
       "        0.6392157 , 0.6745098 , 0.8       , 0.7764706 , 0.7764706 ,\n",
       "        0.7882353 , 0.78039217, 0.78431374, 0.7882353 , 0.78039217,\n",
       "        0.78431374, 0.76862746, 0.7921569 , 0.79607844, 0.7921569 ,\n",
       "        0.79607844, 0.80784315, 0.79607844, 0.8       , 0.79607844,\n",
       "        0.8117647 , 0.83137256, 0.8       , 0.7647059 , 0.76862746,\n",
       "        0.65882355, 0.70980394, 0.4745098 , 0.627451  , 0.63529414,\n",
       "        0.59607846, 0.7254902 , 0.5647059 , 0.7607843 , 0.7764706 ,\n",
       "        0.77254903, 0.77254903, 0.77254903, 0.7607843 , 0.76862746,\n",
       "        0.7764706 , 0.7921569 , 0.78431374, 0.80784315, 0.8156863 ,\n",
       "        0.80784315, 0.8117647 , 0.8039216 , 0.79607844, 0.79607844,\n",
       "        0.78431374, 0.73333335, 0.8156863 , 0.6862745 , 0.76862746,\n",
       "        0.5137255 , 0.6156863 , 0.5803922 , 0.4862745 , 0.627451  ,\n",
       "        0.6627451 , 0.7490196 , 0.64705884, 0.75686276, 0.76862746,\n",
       "        0.75686276, 0.76862746, 0.76862746, 0.78431374, 0.7647059 ,\n",
       "        0.77254903, 0.78431374, 0.7882353 , 0.81960785, 0.79607844,\n",
       "        0.79607844, 0.8039216 , 0.8       , 0.77254903, 0.78431374,\n",
       "        0.7882353 , 0.5254902 , 0.75686276, 0.8156863 , 0.7019608 ,\n",
       "        0.61960787, 0.5019608 , 0.4627451 , 0.79607844, 0.78039217,\n",
       "        0.69803923, 0.7529412 , 0.67058825, 0.7647059 , 0.75686276,\n",
       "        0.7607843 , 0.7647059 , 0.76862746, 0.7607843 , 0.7607843 ,\n",
       "        0.78431374, 0.79607844, 0.78431374, 0.80784315, 0.7921569 ,\n",
       "        0.80784315, 0.8156863 , 0.78039217, 0.79607844, 0.7882353 ,\n",
       "        0.7294118 , 0.78039217, 0.5882353 , 0.7764706 , 0.60784316,\n",
       "        0.5529412 , 0.7647059 , 0.8156863 , 0.84705883, 0.827451  ,\n",
       "        0.7764706 , 0.69411767, 0.6509804 , 0.7647059 , 0.7490196 ,\n",
       "        0.75686276, 0.7529412 , 0.7607843 , 0.7607843 , 0.7607843 ,\n",
       "        0.78431374, 0.78039217, 0.80784315, 0.8       , 0.8039216 ,\n",
       "        0.8039216 , 0.8117647 , 0.79607844, 0.7882353 , 0.79607844,\n",
       "        0.78431374, 0.7607843 , 0.7882353 , 0.6901961 , 0.7176471 ,\n",
       "        0.7607843 , 0.83137256, 0.827451  , 0.8352941 , 0.79607844,\n",
       "        0.7058824 , 0.5764706 , 0.42745098, 0.73333335, 0.7607843 ,\n",
       "        0.7529412 , 0.7607843 , 0.76862746, 0.76862746, 0.7607843 ,\n",
       "        0.78039217, 0.78039217, 0.79607844, 0.8235294 , 0.8       ,\n",
       "        0.8117647 , 0.7921569 , 0.7921569 , 0.7921569 , 0.79607844,\n",
       "        0.7764706 , 0.7176471 , 0.76862746, 0.7882353 , 0.78039217,\n",
       "        0.76862746, 0.7882353 , 0.77254903, 0.78039217, 0.6862745 ,\n",
       "        0.59607846, 0.46666667, 0.5019608 , 0.69411767, 0.7529412 ,\n",
       "        0.7411765 , 0.7647059 , 0.77254903, 0.7607843 , 0.7764706 ,\n",
       "        0.78431374, 0.79607844, 0.79607844, 0.8117647 , 0.8156863 ,\n",
       "        0.8117647 , 0.7882353 , 0.78431374, 0.7921569 , 0.7882353 ,\n",
       "        0.7764706 , 0.67058825, 0.7176471 , 0.7372549 , 0.74509805,\n",
       "        0.7294118 , 0.7607843 , 0.6313726 , 0.6431373 , 0.63529414,\n",
       "        0.5137255 , 0.30588236, 0.4627451 , 0.67058825, 0.75686276,\n",
       "        0.7529412 , 0.76862746, 0.76862746, 0.75686276, 0.74509805,\n",
       "        0.78039217, 0.8       , 0.80784315, 0.8117647 , 0.8156863 ,\n",
       "        0.7921569 , 0.7882353 , 0.7882353 , 0.7882353 , 0.78039217,\n",
       "        0.78039217, 0.5686275 , 0.65882355, 0.6901961 , 0.6784314 ,\n",
       "        0.7372549 , 0.7647059 , 0.7921569 , 0.54901963, 0.4627451 ,\n",
       "        0.34901962, 0.22745098, 0.4745098 , 0.654902  , 0.73333335,\n",
       "        0.7607843 , 0.74509805, 0.7490196 , 0.7411765 , 0.7372549 ,\n",
       "        0.7647059 , 0.79607844, 0.7882353 , 0.7921569 , 0.8       ,\n",
       "        0.7921569 , 0.7882353 , 0.78039217, 0.7882353 , 0.79607844,\n",
       "        0.79607844, 0.6431373 , 0.5686275 , 0.6862745 , 0.7490196 ,\n",
       "        0.6745098 , 0.6745098 , 0.69411767, 0.70980394, 0.7254902 ,\n",
       "        0.7019608 , 0.2627451 , 0.46666667, 0.6431373 , 0.7529412 ,\n",
       "        0.7490196 , 0.7490196 , 0.73333335, 0.74509805, 0.7411765 ,\n",
       "        0.77254903, 0.8       , 0.8       , 0.80784315, 0.79607844,\n",
       "        0.7921569 , 0.78431374, 0.7764706 , 0.7882353 , 0.80784315,\n",
       "        0.78431374, 0.6862745 , 0.54901963, 0.6627451 , 0.7176471 ,\n",
       "        0.7490196 , 0.7411765 , 0.75686276, 0.76862746, 0.74509805,\n",
       "        0.7372549 , 0.63529414, 0.49411765, 0.6509804 , 0.73333335,\n",
       "        0.74509805, 0.7294118 , 0.7294118 , 0.7372549 , 0.7372549 ,\n",
       "        0.78039217, 0.7764706 , 0.8039216 , 0.8156863 , 0.81960785,\n",
       "        0.78431374, 0.7921569 , 0.7921569 , 0.78039217, 0.78039217,\n",
       "        0.7882353 , 0.7254902 , 0.5372549 , 0.63529414, 0.6666667 ,\n",
       "        0.6784314 , 0.6862745 , 0.74509805, 0.7607843 , 0.75686276,\n",
       "        0.74509805, 0.72156864, 0.5372549 , 0.68235296, 0.7411765 ,\n",
       "        0.7254902 , 0.7254902 , 0.73333335, 0.73333335, 0.7294118 ,\n",
       "        0.78431374, 0.79607844, 0.8039216 , 0.8156863 , 0.79607844,\n",
       "        0.78431374, 0.8       , 0.7921569 , 0.76862746, 0.77254903,\n",
       "        0.7921569 , 0.77254903, 0.61960787, 0.61960787, 0.6666667 ,\n",
       "        0.7294118 , 0.7254902 , 0.7254902 , 0.75686276, 0.7882353 ,\n",
       "        0.78431374, 0.7529412 , 0.6627451 , 0.19607843, 0.7254902 ,\n",
       "        0.72156864, 0.7254902 , 0.73333335, 0.7254902 , 0.72156864,\n",
       "        0.7764706 , 0.7882353 , 0.79607844, 0.8039216 , 0.79607844,\n",
       "        0.7921569 , 0.7882353 , 0.7882353 , 0.80784315, 0.7882353 ,\n",
       "        0.8039216 , 0.77254903, 0.69803923, 0.5411765 , 0.6901961 ,\n",
       "        0.77254903, 0.78431374, 0.73333335, 0.74509805, 0.79607844,\n",
       "        0.79607844, 0.7764706 , 0.2509804 , 0.15686275, 0.17254902,\n",
       "        0.70980394, 0.7137255 , 0.7176471 , 0.7176471 , 0.73333335,\n",
       "        0.77254903, 0.78431374, 0.79607844, 0.7764706 , 0.8117647 ,\n",
       "        0.8235294 , 0.7764706 , 0.8       , 0.7882353 , 0.79607844,\n",
       "        0.7764706 , 0.7764706 , 0.7372549 , 0.59607846, 0.57254905,\n",
       "        0.7529412 , 0.79607844, 0.79607844, 0.80784315, 0.78039217,\n",
       "        0.79607844, 0.69411767, 0.18431373, 0.17254902, 0.17254902,\n",
       "        0.1254902 , 0.7137255 , 0.70980394, 0.70980394, 0.72156864,\n",
       "        0.77254903, 0.8117647 , 0.8039216 , 0.7921569 , 0.79607844,\n",
       "        0.8156863 , 0.8       , 0.8       , 0.78431374, 0.77254903,\n",
       "        0.77254903, 0.75686276, 0.7647059 , 0.64705884, 0.43529412,\n",
       "        0.7058824 , 0.78431374, 0.8156863 , 0.81960785, 0.8117647 ,\n",
       "        0.8117647 , 0.20784314, 0.19607843, 0.18431373, 0.17254902,\n",
       "        0.15686275, 0.18039216, 0.18039216, 0.7137255 , 0.70980394,\n",
       "        0.7882353 , 0.78039217, 0.7882353 , 0.7921569 , 0.79607844,\n",
       "        0.78431374, 0.7882353 , 0.78431374, 0.78431374, 0.78431374,\n",
       "        0.77254903, 0.75686276, 0.75686276, 0.6901961 , 0.54901963,\n",
       "        0.6117647 , 0.73333335, 0.78039217, 0.8156863 , 0.8039216 ,\n",
       "        0.23137255, 0.20784314, 0.20392157, 0.2       , 0.20392157,\n",
       "        0.1882353 , 0.13333334, 0.1764706 , 0.19215687, 0.7019608 ,\n",
       "        0.7764706 , 0.8039216 , 0.7882353 , 0.8       , 0.8039216 ,\n",
       "        0.78039217, 0.79607844, 0.78431374, 0.78431374, 0.78039217,\n",
       "        0.7647059 , 0.7921569 , 0.75686276, 0.7294118 , 0.6039216 ,\n",
       "        0.43529412, 0.6627451 , 0.7529412 , 0.78039217, 0.24313726,\n",
       "        0.20784314, 0.21960784, 0.21960784, 0.12941177, 0.22745098,\n",
       "        0.22745098, 0.21960784, 0.10588235, 0.17254902, 0.6901961 ,\n",
       "        0.78431374, 0.78431374, 0.78431374, 0.79607844, 0.7921569 ,\n",
       "        0.76862746, 0.7921569 , 0.76862746, 0.76862746, 0.78431374,\n",
       "        0.7607843 , 0.7607843 , 0.7529412 , 0.75686276, 0.6745098 ,\n",
       "        0.5294118 , 0.5647059 , 0.7176471 , 0.2509804 , 0.23529412,\n",
       "        0.23529412, 0.22745098, 0.16078432, 0.2509804 , 0.23137255,\n",
       "        0.20392157, 0.20392157, 0.20392157, 0.10196079, 0.6862745 ,\n",
       "        0.78039217, 0.78431374, 0.78039217, 0.7764706 , 0.76862746,\n",
       "        0.7882353 , 0.76862746, 0.78039217, 0.78039217, 0.75686276,\n",
       "        0.75686276, 0.76862746, 0.7529412 , 0.74509805, 0.7019608 ,\n",
       "        0.58431375, 0.14117648, 0.27058825, 0.23137255, 0.20392157,\n",
       "        0.21960784, 0.14901961, 0.23529412, 0.21960784, 0.21960784,\n",
       "        0.20392157, 0.2509804 , 0.09019608, 0.19607843, 0.1882353 ,\n",
       "        0.77254903, 0.78431374, 0.77254903, 0.7764706 , 0.7607843 ,\n",
       "        0.78039217, 0.7764706 , 0.7921569 , 0.76862746, 0.7490196 ,\n",
       "        0.7372549 , 0.75686276, 0.74509805, 0.7411765 , 0.73333335,\n",
       "        0.22352941, 0.23137255, 0.23137255, 0.22745098, 0.22352941,\n",
       "        0.1254902 , 0.23921569, 0.19215687, 0.24705882, 0.21568628,\n",
       "        0.24313726, 0.10588235, 0.19607843, 0.19215687, 0.20392157,\n",
       "        0.7647059 , 0.78039217, 0.78039217, 0.78431374, 0.78039217,\n",
       "        0.7882353 , 0.7764706 , 0.78039217, 0.76862746, 0.7607843 ,\n",
       "        0.7607843 , 0.7529412 , 0.7372549 , 0.7294118 , 0.23921569,\n",
       "        0.25490198, 0.23921569, 0.23137255, 0.22745098, 0.12156863,\n",
       "        0.21960784, 0.15686275, 0.2627451 , 0.23921569, 0.22352941,\n",
       "        0.15294118, 0.20784314, 0.20784314, 0.21568628, 0.22745098,\n",
       "        0.77254903, 0.78039217, 0.7764706 , 0.78039217, 0.7764706 ,\n",
       "        0.7882353 , 0.78039217, 0.7647059 , 0.7607843 , 0.7647059 ,\n",
       "        0.74509805, 0.7490196 , 0.7490196 , 0.41568628, 0.22745098,\n",
       "        0.24313726, 0.23921569, 0.21960784, 0.16470589, 0.1764706 ,\n",
       "        0.17254902, 0.26666668, 0.25490198, 0.22352941, 0.20784314,\n",
       "        0.22745098, 0.22745098, 0.21960784, 0.22352941, 0.23137255,\n",
       "        0.78039217, 0.78039217, 0.78431374, 0.78431374, 0.78039217,\n",
       "        0.76862746, 0.76862746, 0.76862746, 0.7607843 , 0.7607843 ,\n",
       "        0.75686276, 0.7490196 , 0.69411767, 0.23529412, 0.23529412,\n",
       "        0.22352941, 0.21960784, 0.16470589, 0.14509805, 0.1882353 ,\n",
       "        0.2784314 , 0.2509804 , 0.1764706 , 0.22352941, 0.23529412,\n",
       "        0.23137255, 0.21960784, 0.21176471, 0.24705882, 0.23137255]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.expand_dims(X_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d60834b518>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0,:,:], cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (4658, 900)\n"
     ]
    }
   ],
   "source": [
    "ndims = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], ndims)\n",
    "print(\"Training Shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               90100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 26)                2626      \n",
      "=================================================================\n",
      "Total params: 102,826\n",
      "Trainable params: 102,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-70-64affaec36d7>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-70-64affaec36d7>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    verbose=2\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    shuffle=True\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 4658 input samples and 26 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-88726871a517>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     verbose=2)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2532\u001b[0m       \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2533\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2534\u001b[1;33m         \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2535\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2536\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    675\u001b[0m                      \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m                      \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 4658 input samples and 26 target samples."
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sign_trained2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we must create test data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.xception import (Xception, preprocess_input, decode_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"sign_trained2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d605fdee10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = os.path.join(\"..\", \"Sign\", \"test2.jpg\")\n",
    "img = image.load_img(image_path, target_size=(30,30), color_mode=\"grayscale\")\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[205.]\n",
      "  [205.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [207.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [202.]\n",
      "  [202.]\n",
      "  [204.]\n",
      "  [204.]\n",
      "  [199.]\n",
      "  [202.]\n",
      "  [200.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [200.]\n",
      "  [203.]]\n",
      "\n",
      " [[205.]\n",
      "  [203.]\n",
      "  [208.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [203.]\n",
      "  [204.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [200.]\n",
      "  [199.]\n",
      "  [206.]\n",
      "  [202.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [199.]\n",
      "  [199.]\n",
      "  [204.]\n",
      "  [201.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [205.]]\n",
      "\n",
      " [[203.]\n",
      "  [205.]\n",
      "  [204.]\n",
      "  [207.]\n",
      "  [206.]\n",
      "  [204.]\n",
      "  [209.]\n",
      "  [205.]\n",
      "  [207.]\n",
      "  [206.]\n",
      "  [206.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [203.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [199.]\n",
      "  [203.]\n",
      "  [202.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [202.]\n",
      "  [205.]\n",
      "  [202.]\n",
      "  [200.]\n",
      "  [203.]]\n",
      "\n",
      " [[203.]\n",
      "  [203.]\n",
      "  [204.]\n",
      "  [208.]\n",
      "  [205.]\n",
      "  [204.]\n",
      "  [212.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [208.]\n",
      "  [207.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [204.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [202.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [199.]\n",
      "  [200.]\n",
      "  [200.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [197.]\n",
      "  [200.]\n",
      "  [203.]\n",
      "  [200.]]\n",
      "\n",
      " [[203.]\n",
      "  [203.]\n",
      "  [204.]\n",
      "  [202.]\n",
      "  [205.]\n",
      "  [207.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [206.]\n",
      "  [206.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [213.]\n",
      "  [209.]\n",
      "  [204.]\n",
      "  [199.]\n",
      "  [191.]\n",
      "  [172.]\n",
      "  [122.]\n",
      "  [201.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [200.]\n",
      "  [200.]\n",
      "  [203.]\n",
      "  [200.]\n",
      "  [200.]]\n",
      "\n",
      " [[202.]\n",
      "  [199.]\n",
      "  [204.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [204.]\n",
      "  [210.]\n",
      "  [206.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [206.]\n",
      "  [207.]\n",
      "  [210.]\n",
      "  [202.]\n",
      "  [205.]\n",
      "  [202.]\n",
      "  [170.]\n",
      "  [203.]\n",
      "  [188.]\n",
      "  [176.]\n",
      "  [174.]\n",
      "  [197.]\n",
      "  [199.]\n",
      "  [203.]\n",
      "  [201.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [199.]\n",
      "  [205.]]\n",
      "\n",
      " [[202.]\n",
      "  [202.]\n",
      "  [203.]\n",
      "  [204.]\n",
      "  [203.]\n",
      "  [212.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [207.]\n",
      "  [206.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [192.]\n",
      "  [202.]\n",
      "  [208.]\n",
      "  [201.]\n",
      "  [198.]\n",
      "  [149.]\n",
      "  [185.]\n",
      "  [174.]\n",
      "  [163.]\n",
      "  [172.]\n",
      "  [204.]\n",
      "  [198.]\n",
      "  [198.]\n",
      "  [201.]\n",
      "  [199.]\n",
      "  [200.]\n",
      "  [201.]\n",
      "  [199.]]\n",
      "\n",
      " [[200.]\n",
      "  [196.]\n",
      "  [202.]\n",
      "  [203.]\n",
      "  [202.]\n",
      "  [203.]\n",
      "  [206.]\n",
      "  [203.]\n",
      "  [204.]\n",
      "  [203.]\n",
      "  [207.]\n",
      "  [212.]\n",
      "  [204.]\n",
      "  [195.]\n",
      "  [196.]\n",
      "  [168.]\n",
      "  [181.]\n",
      "  [121.]\n",
      "  [160.]\n",
      "  [162.]\n",
      "  [152.]\n",
      "  [185.]\n",
      "  [144.]\n",
      "  [194.]\n",
      "  [198.]\n",
      "  [197.]\n",
      "  [197.]\n",
      "  [197.]\n",
      "  [194.]\n",
      "  [196.]]\n",
      "\n",
      " [[198.]\n",
      "  [202.]\n",
      "  [200.]\n",
      "  [206.]\n",
      "  [208.]\n",
      "  [206.]\n",
      "  [207.]\n",
      "  [205.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [200.]\n",
      "  [187.]\n",
      "  [208.]\n",
      "  [175.]\n",
      "  [196.]\n",
      "  [131.]\n",
      "  [157.]\n",
      "  [148.]\n",
      "  [124.]\n",
      "  [160.]\n",
      "  [169.]\n",
      "  [191.]\n",
      "  [165.]\n",
      "  [193.]\n",
      "  [196.]\n",
      "  [193.]\n",
      "  [196.]\n",
      "  [196.]\n",
      "  [200.]\n",
      "  [195.]]\n",
      "\n",
      " [[197.]\n",
      "  [200.]\n",
      "  [201.]\n",
      "  [209.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [204.]\n",
      "  [197.]\n",
      "  [200.]\n",
      "  [201.]\n",
      "  [134.]\n",
      "  [193.]\n",
      "  [208.]\n",
      "  [179.]\n",
      "  [158.]\n",
      "  [128.]\n",
      "  [118.]\n",
      "  [203.]\n",
      "  [199.]\n",
      "  [178.]\n",
      "  [192.]\n",
      "  [171.]\n",
      "  [195.]\n",
      "  [193.]\n",
      "  [194.]\n",
      "  [195.]\n",
      "  [196.]\n",
      "  [194.]\n",
      "  [194.]]\n",
      "\n",
      " [[200.]\n",
      "  [203.]\n",
      "  [200.]\n",
      "  [206.]\n",
      "  [202.]\n",
      "  [206.]\n",
      "  [208.]\n",
      "  [199.]\n",
      "  [203.]\n",
      "  [201.]\n",
      "  [186.]\n",
      "  [199.]\n",
      "  [150.]\n",
      "  [198.]\n",
      "  [155.]\n",
      "  [141.]\n",
      "  [195.]\n",
      "  [208.]\n",
      "  [216.]\n",
      "  [211.]\n",
      "  [198.]\n",
      "  [177.]\n",
      "  [166.]\n",
      "  [195.]\n",
      "  [191.]\n",
      "  [193.]\n",
      "  [192.]\n",
      "  [194.]\n",
      "  [194.]\n",
      "  [194.]]\n",
      "\n",
      " [[200.]\n",
      "  [199.]\n",
      "  [206.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [205.]\n",
      "  [207.]\n",
      "  [203.]\n",
      "  [201.]\n",
      "  [203.]\n",
      "  [200.]\n",
      "  [194.]\n",
      "  [201.]\n",
      "  [176.]\n",
      "  [183.]\n",
      "  [194.]\n",
      "  [212.]\n",
      "  [211.]\n",
      "  [213.]\n",
      "  [203.]\n",
      "  [180.]\n",
      "  [147.]\n",
      "  [109.]\n",
      "  [187.]\n",
      "  [194.]\n",
      "  [192.]\n",
      "  [194.]\n",
      "  [196.]\n",
      "  [196.]\n",
      "  [194.]]\n",
      "\n",
      " [[199.]\n",
      "  [199.]\n",
      "  [203.]\n",
      "  [210.]\n",
      "  [204.]\n",
      "  [207.]\n",
      "  [202.]\n",
      "  [202.]\n",
      "  [202.]\n",
      "  [203.]\n",
      "  [198.]\n",
      "  [183.]\n",
      "  [196.]\n",
      "  [201.]\n",
      "  [199.]\n",
      "  [196.]\n",
      "  [201.]\n",
      "  [197.]\n",
      "  [199.]\n",
      "  [175.]\n",
      "  [152.]\n",
      "  [119.]\n",
      "  [128.]\n",
      "  [177.]\n",
      "  [192.]\n",
      "  [189.]\n",
      "  [195.]\n",
      "  [197.]\n",
      "  [194.]\n",
      "  [198.]]\n",
      "\n",
      " [[200.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [207.]\n",
      "  [208.]\n",
      "  [207.]\n",
      "  [201.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [201.]\n",
      "  [198.]\n",
      "  [171.]\n",
      "  [183.]\n",
      "  [188.]\n",
      "  [190.]\n",
      "  [186.]\n",
      "  [194.]\n",
      "  [161.]\n",
      "  [164.]\n",
      "  [162.]\n",
      "  [131.]\n",
      "  [ 78.]\n",
      "  [118.]\n",
      "  [171.]\n",
      "  [193.]\n",
      "  [192.]\n",
      "  [196.]\n",
      "  [196.]\n",
      "  [193.]\n",
      "  [190.]]\n",
      "\n",
      " [[199.]\n",
      "  [204.]\n",
      "  [206.]\n",
      "  [207.]\n",
      "  [208.]\n",
      "  [202.]\n",
      "  [201.]\n",
      "  [201.]\n",
      "  [201.]\n",
      "  [199.]\n",
      "  [199.]\n",
      "  [145.]\n",
      "  [168.]\n",
      "  [176.]\n",
      "  [173.]\n",
      "  [188.]\n",
      "  [195.]\n",
      "  [202.]\n",
      "  [140.]\n",
      "  [118.]\n",
      "  [ 89.]\n",
      "  [ 58.]\n",
      "  [121.]\n",
      "  [167.]\n",
      "  [187.]\n",
      "  [194.]\n",
      "  [190.]\n",
      "  [191.]\n",
      "  [189.]\n",
      "  [188.]]\n",
      "\n",
      " [[195.]\n",
      "  [203.]\n",
      "  [201.]\n",
      "  [202.]\n",
      "  [204.]\n",
      "  [202.]\n",
      "  [201.]\n",
      "  [199.]\n",
      "  [201.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [164.]\n",
      "  [145.]\n",
      "  [175.]\n",
      "  [191.]\n",
      "  [172.]\n",
      "  [172.]\n",
      "  [177.]\n",
      "  [181.]\n",
      "  [185.]\n",
      "  [179.]\n",
      "  [ 67.]\n",
      "  [119.]\n",
      "  [164.]\n",
      "  [192.]\n",
      "  [191.]\n",
      "  [191.]\n",
      "  [187.]\n",
      "  [190.]\n",
      "  [189.]]\n",
      "\n",
      " [[197.]\n",
      "  [204.]\n",
      "  [204.]\n",
      "  [206.]\n",
      "  [203.]\n",
      "  [202.]\n",
      "  [200.]\n",
      "  [198.]\n",
      "  [201.]\n",
      "  [206.]\n",
      "  [200.]\n",
      "  [175.]\n",
      "  [140.]\n",
      "  [169.]\n",
      "  [183.]\n",
      "  [191.]\n",
      "  [189.]\n",
      "  [193.]\n",
      "  [196.]\n",
      "  [190.]\n",
      "  [188.]\n",
      "  [162.]\n",
      "  [126.]\n",
      "  [166.]\n",
      "  [187.]\n",
      "  [190.]\n",
      "  [186.]\n",
      "  [186.]\n",
      "  [188.]\n",
      "  [188.]]\n",
      "\n",
      " [[199.]\n",
      "  [198.]\n",
      "  [205.]\n",
      "  [208.]\n",
      "  [209.]\n",
      "  [200.]\n",
      "  [202.]\n",
      "  [202.]\n",
      "  [199.]\n",
      "  [199.]\n",
      "  [201.]\n",
      "  [185.]\n",
      "  [137.]\n",
      "  [162.]\n",
      "  [170.]\n",
      "  [173.]\n",
      "  [175.]\n",
      "  [190.]\n",
      "  [194.]\n",
      "  [193.]\n",
      "  [190.]\n",
      "  [184.]\n",
      "  [137.]\n",
      "  [174.]\n",
      "  [189.]\n",
      "  [185.]\n",
      "  [185.]\n",
      "  [187.]\n",
      "  [187.]\n",
      "  [186.]]\n",
      "\n",
      " [[200.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [208.]\n",
      "  [203.]\n",
      "  [200.]\n",
      "  [204.]\n",
      "  [202.]\n",
      "  [196.]\n",
      "  [197.]\n",
      "  [202.]\n",
      "  [197.]\n",
      "  [158.]\n",
      "  [158.]\n",
      "  [170.]\n",
      "  [186.]\n",
      "  [185.]\n",
      "  [185.]\n",
      "  [193.]\n",
      "  [201.]\n",
      "  [200.]\n",
      "  [192.]\n",
      "  [169.]\n",
      "  [ 50.]\n",
      "  [185.]\n",
      "  [184.]\n",
      "  [185.]\n",
      "  [187.]\n",
      "  [185.]\n",
      "  [184.]]\n",
      "\n",
      " [[198.]\n",
      "  [201.]\n",
      "  [203.]\n",
      "  [205.]\n",
      "  [203.]\n",
      "  [202.]\n",
      "  [201.]\n",
      "  [201.]\n",
      "  [206.]\n",
      "  [201.]\n",
      "  [205.]\n",
      "  [197.]\n",
      "  [178.]\n",
      "  [138.]\n",
      "  [176.]\n",
      "  [197.]\n",
      "  [200.]\n",
      "  [187.]\n",
      "  [190.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [198.]\n",
      "  [ 64.]\n",
      "  [ 40.]\n",
      "  [ 44.]\n",
      "  [181.]\n",
      "  [182.]\n",
      "  [183.]\n",
      "  [183.]\n",
      "  [187.]]\n",
      "\n",
      " [[197.]\n",
      "  [200.]\n",
      "  [203.]\n",
      "  [198.]\n",
      "  [207.]\n",
      "  [210.]\n",
      "  [198.]\n",
      "  [204.]\n",
      "  [201.]\n",
      "  [203.]\n",
      "  [198.]\n",
      "  [198.]\n",
      "  [188.]\n",
      "  [152.]\n",
      "  [146.]\n",
      "  [192.]\n",
      "  [203.]\n",
      "  [203.]\n",
      "  [206.]\n",
      "  [199.]\n",
      "  [203.]\n",
      "  [177.]\n",
      "  [ 47.]\n",
      "  [ 44.]\n",
      "  [ 44.]\n",
      "  [ 32.]\n",
      "  [182.]\n",
      "  [181.]\n",
      "  [181.]\n",
      "  [184.]]\n",
      "\n",
      " [[197.]\n",
      "  [207.]\n",
      "  [205.]\n",
      "  [202.]\n",
      "  [203.]\n",
      "  [208.]\n",
      "  [204.]\n",
      "  [204.]\n",
      "  [200.]\n",
      "  [197.]\n",
      "  [197.]\n",
      "  [193.]\n",
      "  [195.]\n",
      "  [165.]\n",
      "  [111.]\n",
      "  [180.]\n",
      "  [200.]\n",
      "  [208.]\n",
      "  [209.]\n",
      "  [207.]\n",
      "  [207.]\n",
      "  [ 53.]\n",
      "  [ 50.]\n",
      "  [ 47.]\n",
      "  [ 44.]\n",
      "  [ 40.]\n",
      "  [ 46.]\n",
      "  [ 46.]\n",
      "  [182.]\n",
      "  [181.]]\n",
      "\n",
      " [[201.]\n",
      "  [199.]\n",
      "  [201.]\n",
      "  [202.]\n",
      "  [203.]\n",
      "  [200.]\n",
      "  [201.]\n",
      "  [200.]\n",
      "  [200.]\n",
      "  [200.]\n",
      "  [197.]\n",
      "  [193.]\n",
      "  [193.]\n",
      "  [176.]\n",
      "  [140.]\n",
      "  [156.]\n",
      "  [187.]\n",
      "  [199.]\n",
      "  [208.]\n",
      "  [205.]\n",
      "  [ 59.]\n",
      "  [ 53.]\n",
      "  [ 52.]\n",
      "  [ 51.]\n",
      "  [ 52.]\n",
      "  [ 48.]\n",
      "  [ 34.]\n",
      "  [ 45.]\n",
      "  [ 49.]\n",
      "  [179.]]\n",
      "\n",
      " [[198.]\n",
      "  [205.]\n",
      "  [201.]\n",
      "  [204.]\n",
      "  [205.]\n",
      "  [199.]\n",
      "  [203.]\n",
      "  [200.]\n",
      "  [200.]\n",
      "  [199.]\n",
      "  [195.]\n",
      "  [202.]\n",
      "  [193.]\n",
      "  [186.]\n",
      "  [154.]\n",
      "  [111.]\n",
      "  [169.]\n",
      "  [192.]\n",
      "  [199.]\n",
      "  [ 62.]\n",
      "  [ 53.]\n",
      "  [ 56.]\n",
      "  [ 56.]\n",
      "  [ 33.]\n",
      "  [ 58.]\n",
      "  [ 58.]\n",
      "  [ 56.]\n",
      "  [ 27.]\n",
      "  [ 44.]\n",
      "  [176.]]\n",
      "\n",
      " [[200.]\n",
      "  [200.]\n",
      "  [200.]\n",
      "  [203.]\n",
      "  [202.]\n",
      "  [196.]\n",
      "  [202.]\n",
      "  [196.]\n",
      "  [196.]\n",
      "  [200.]\n",
      "  [194.]\n",
      "  [194.]\n",
      "  [192.]\n",
      "  [193.]\n",
      "  [172.]\n",
      "  [135.]\n",
      "  [144.]\n",
      "  [183.]\n",
      "  [ 64.]\n",
      "  [ 60.]\n",
      "  [ 60.]\n",
      "  [ 58.]\n",
      "  [ 41.]\n",
      "  [ 64.]\n",
      "  [ 59.]\n",
      "  [ 52.]\n",
      "  [ 52.]\n",
      "  [ 52.]\n",
      "  [ 26.]\n",
      "  [175.]]\n",
      "\n",
      " [[199.]\n",
      "  [200.]\n",
      "  [199.]\n",
      "  [198.]\n",
      "  [196.]\n",
      "  [201.]\n",
      "  [196.]\n",
      "  [199.]\n",
      "  [199.]\n",
      "  [193.]\n",
      "  [193.]\n",
      "  [196.]\n",
      "  [192.]\n",
      "  [190.]\n",
      "  [179.]\n",
      "  [149.]\n",
      "  [ 36.]\n",
      "  [ 69.]\n",
      "  [ 59.]\n",
      "  [ 52.]\n",
      "  [ 56.]\n",
      "  [ 38.]\n",
      "  [ 60.]\n",
      "  [ 56.]\n",
      "  [ 56.]\n",
      "  [ 52.]\n",
      "  [ 64.]\n",
      "  [ 23.]\n",
      "  [ 50.]\n",
      "  [ 48.]]\n",
      "\n",
      " [[197.]\n",
      "  [200.]\n",
      "  [197.]\n",
      "  [198.]\n",
      "  [194.]\n",
      "  [199.]\n",
      "  [198.]\n",
      "  [202.]\n",
      "  [196.]\n",
      "  [191.]\n",
      "  [188.]\n",
      "  [193.]\n",
      "  [190.]\n",
      "  [189.]\n",
      "  [187.]\n",
      "  [ 57.]\n",
      "  [ 59.]\n",
      "  [ 59.]\n",
      "  [ 58.]\n",
      "  [ 57.]\n",
      "  [ 32.]\n",
      "  [ 61.]\n",
      "  [ 49.]\n",
      "  [ 63.]\n",
      "  [ 55.]\n",
      "  [ 62.]\n",
      "  [ 27.]\n",
      "  [ 50.]\n",
      "  [ 49.]\n",
      "  [ 52.]]\n",
      "\n",
      " [[195.]\n",
      "  [199.]\n",
      "  [199.]\n",
      "  [200.]\n",
      "  [199.]\n",
      "  [201.]\n",
      "  [198.]\n",
      "  [199.]\n",
      "  [196.]\n",
      "  [194.]\n",
      "  [194.]\n",
      "  [192.]\n",
      "  [188.]\n",
      "  [186.]\n",
      "  [ 61.]\n",
      "  [ 65.]\n",
      "  [ 61.]\n",
      "  [ 59.]\n",
      "  [ 58.]\n",
      "  [ 31.]\n",
      "  [ 56.]\n",
      "  [ 40.]\n",
      "  [ 67.]\n",
      "  [ 61.]\n",
      "  [ 57.]\n",
      "  [ 39.]\n",
      "  [ 53.]\n",
      "  [ 53.]\n",
      "  [ 55.]\n",
      "  [ 58.]]\n",
      "\n",
      " [[197.]\n",
      "  [199.]\n",
      "  [198.]\n",
      "  [199.]\n",
      "  [198.]\n",
      "  [201.]\n",
      "  [199.]\n",
      "  [195.]\n",
      "  [194.]\n",
      "  [195.]\n",
      "  [190.]\n",
      "  [191.]\n",
      "  [191.]\n",
      "  [106.]\n",
      "  [ 58.]\n",
      "  [ 62.]\n",
      "  [ 61.]\n",
      "  [ 56.]\n",
      "  [ 42.]\n",
      "  [ 45.]\n",
      "  [ 44.]\n",
      "  [ 68.]\n",
      "  [ 65.]\n",
      "  [ 57.]\n",
      "  [ 53.]\n",
      "  [ 58.]\n",
      "  [ 58.]\n",
      "  [ 56.]\n",
      "  [ 57.]\n",
      "  [ 59.]]\n",
      "\n",
      " [[199.]\n",
      "  [199.]\n",
      "  [200.]\n",
      "  [200.]\n",
      "  [199.]\n",
      "  [196.]\n",
      "  [196.]\n",
      "  [196.]\n",
      "  [194.]\n",
      "  [194.]\n",
      "  [193.]\n",
      "  [191.]\n",
      "  [177.]\n",
      "  [ 60.]\n",
      "  [ 60.]\n",
      "  [ 57.]\n",
      "  [ 56.]\n",
      "  [ 42.]\n",
      "  [ 37.]\n",
      "  [ 48.]\n",
      "  [ 71.]\n",
      "  [ 64.]\n",
      "  [ 45.]\n",
      "  [ 57.]\n",
      "  [ 60.]\n",
      "  [ 59.]\n",
      "  [ 56.]\n",
      "  [ 54.]\n",
      "  [ 63.]\n",
      "  [ 59.]]]\n"
     ]
    }
   ],
   "source": [
    "cat = img_to_array(img)\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[205. 205. 203. 203. 205. 205. 205. 207. 204. 205. 203. 203. 205. 205.\n",
      "  204. 205. 202. 202. 204. 204. 199. 202. 200. 203. 205. 203. 203. 205.\n",
      "  200. 203.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(cat[0]).reshape(1,-1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(cat) \n",
    "cat = np.array(cat).reshape(1,-1)\n",
    "cat = cat / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8039216 , 0.8039216 , 0.79607844, 0.79607844, 0.8039216 ,\n",
       "        0.8039216 , 0.8039216 , 0.8117647 , 0.8       , 0.8039216 ,\n",
       "        0.79607844, 0.79607844, 0.8039216 , 0.8039216 , 0.8       ,\n",
       "        0.8039216 , 0.7921569 , 0.7921569 , 0.8       , 0.8       ,\n",
       "        0.78039217, 0.7921569 , 0.78431374, 0.79607844, 0.8039216 ,\n",
       "        0.79607844, 0.79607844, 0.8039216 , 0.78431374, 0.79607844,\n",
       "        0.8039216 , 0.79607844, 0.8156863 , 0.8039216 , 0.80784315,\n",
       "        0.8       , 0.8039216 , 0.8039216 , 0.8       , 0.8039216 ,\n",
       "        0.8039216 , 0.79607844, 0.8       , 0.8       , 0.8039216 ,\n",
       "        0.78431374, 0.78039217, 0.80784315, 0.7921569 , 0.78431374,\n",
       "        0.7921569 , 0.78039217, 0.78039217, 0.8       , 0.7882353 ,\n",
       "        0.79607844, 0.8039216 , 0.79607844, 0.79607844, 0.8039216 ,\n",
       "        0.79607844, 0.8039216 , 0.8       , 0.8117647 , 0.80784315,\n",
       "        0.8       , 0.81960785, 0.8039216 , 0.8117647 , 0.80784315,\n",
       "        0.80784315, 0.8039216 , 0.80784315, 0.8039216 , 0.8039216 ,\n",
       "        0.8039216 , 0.8039216 , 0.79607844, 0.78431374, 0.7921569 ,\n",
       "        0.78039217, 0.79607844, 0.7921569 , 0.78431374, 0.7921569 ,\n",
       "        0.7921569 , 0.8039216 , 0.7921569 , 0.78431374, 0.79607844,\n",
       "        0.79607844, 0.79607844, 0.8       , 0.8156863 , 0.8039216 ,\n",
       "        0.8       , 0.83137256, 0.8039216 , 0.8039216 , 0.8039216 ,\n",
       "        0.80784315, 0.8156863 , 0.8117647 , 0.8039216 , 0.80784315,\n",
       "        0.8       , 0.78431374, 0.7921569 , 0.7921569 , 0.78431374,\n",
       "        0.7921569 , 0.78039217, 0.78431374, 0.78431374, 0.78431374,\n",
       "        0.7921569 , 0.77254903, 0.78431374, 0.79607844, 0.78431374,\n",
       "        0.79607844, 0.79607844, 0.8       , 0.7921569 , 0.8039216 ,\n",
       "        0.8117647 , 0.8039216 , 0.8039216 , 0.80784315, 0.80784315,\n",
       "        0.80784315, 0.8039216 , 0.80784315, 0.8039216 , 0.80784315,\n",
       "        0.8352941 , 0.81960785, 0.8       , 0.78039217, 0.7490196 ,\n",
       "        0.6745098 , 0.47843137, 0.7882353 , 0.78431374, 0.7921569 ,\n",
       "        0.78431374, 0.78431374, 0.79607844, 0.78431374, 0.78431374,\n",
       "        0.7921569 , 0.78039217, 0.8       , 0.8       , 0.8039216 ,\n",
       "        0.8       , 0.8235294 , 0.80784315, 0.79607844, 0.8039216 ,\n",
       "        0.8039216 , 0.80784315, 0.8117647 , 0.8235294 , 0.7921569 ,\n",
       "        0.8039216 , 0.7921569 , 0.6666667 , 0.79607844, 0.7372549 ,\n",
       "        0.6901961 , 0.68235296, 0.77254903, 0.78039217, 0.79607844,\n",
       "        0.7882353 , 0.78431374, 0.7921569 , 0.78039217, 0.8039216 ,\n",
       "        0.7921569 , 0.7921569 , 0.79607844, 0.8       , 0.79607844,\n",
       "        0.83137256, 0.79607844, 0.8039216 , 0.8117647 , 0.80784315,\n",
       "        0.79607844, 0.8039216 , 0.7529412 , 0.7921569 , 0.8156863 ,\n",
       "        0.7882353 , 0.7764706 , 0.58431375, 0.7254902 , 0.68235296,\n",
       "        0.6392157 , 0.6745098 , 0.8       , 0.7764706 , 0.7764706 ,\n",
       "        0.7882353 , 0.78039217, 0.78431374, 0.7882353 , 0.78039217,\n",
       "        0.78431374, 0.76862746, 0.7921569 , 0.79607844, 0.7921569 ,\n",
       "        0.79607844, 0.80784315, 0.79607844, 0.8       , 0.79607844,\n",
       "        0.8117647 , 0.83137256, 0.8       , 0.7647059 , 0.76862746,\n",
       "        0.65882355, 0.70980394, 0.4745098 , 0.627451  , 0.63529414,\n",
       "        0.59607846, 0.7254902 , 0.5647059 , 0.7607843 , 0.7764706 ,\n",
       "        0.77254903, 0.77254903, 0.77254903, 0.7607843 , 0.76862746,\n",
       "        0.7764706 , 0.7921569 , 0.78431374, 0.80784315, 0.8156863 ,\n",
       "        0.80784315, 0.8117647 , 0.8039216 , 0.79607844, 0.79607844,\n",
       "        0.78431374, 0.73333335, 0.8156863 , 0.6862745 , 0.76862746,\n",
       "        0.5137255 , 0.6156863 , 0.5803922 , 0.4862745 , 0.627451  ,\n",
       "        0.6627451 , 0.7490196 , 0.64705884, 0.75686276, 0.76862746,\n",
       "        0.75686276, 0.76862746, 0.76862746, 0.78431374, 0.7647059 ,\n",
       "        0.77254903, 0.78431374, 0.7882353 , 0.81960785, 0.79607844,\n",
       "        0.79607844, 0.8039216 , 0.8       , 0.77254903, 0.78431374,\n",
       "        0.7882353 , 0.5254902 , 0.75686276, 0.8156863 , 0.7019608 ,\n",
       "        0.61960787, 0.5019608 , 0.4627451 , 0.79607844, 0.78039217,\n",
       "        0.69803923, 0.7529412 , 0.67058825, 0.7647059 , 0.75686276,\n",
       "        0.7607843 , 0.7647059 , 0.76862746, 0.7607843 , 0.7607843 ,\n",
       "        0.78431374, 0.79607844, 0.78431374, 0.80784315, 0.7921569 ,\n",
       "        0.80784315, 0.8156863 , 0.78039217, 0.79607844, 0.7882353 ,\n",
       "        0.7294118 , 0.78039217, 0.5882353 , 0.7764706 , 0.60784316,\n",
       "        0.5529412 , 0.7647059 , 0.8156863 , 0.84705883, 0.827451  ,\n",
       "        0.7764706 , 0.69411767, 0.6509804 , 0.7647059 , 0.7490196 ,\n",
       "        0.75686276, 0.7529412 , 0.7607843 , 0.7607843 , 0.7607843 ,\n",
       "        0.78431374, 0.78039217, 0.80784315, 0.8       , 0.8039216 ,\n",
       "        0.8039216 , 0.8117647 , 0.79607844, 0.7882353 , 0.79607844,\n",
       "        0.78431374, 0.7607843 , 0.7882353 , 0.6901961 , 0.7176471 ,\n",
       "        0.7607843 , 0.83137256, 0.827451  , 0.8352941 , 0.79607844,\n",
       "        0.7058824 , 0.5764706 , 0.42745098, 0.73333335, 0.7607843 ,\n",
       "        0.7529412 , 0.7607843 , 0.76862746, 0.76862746, 0.7607843 ,\n",
       "        0.78039217, 0.78039217, 0.79607844, 0.8235294 , 0.8       ,\n",
       "        0.8117647 , 0.7921569 , 0.7921569 , 0.7921569 , 0.79607844,\n",
       "        0.7764706 , 0.7176471 , 0.76862746, 0.7882353 , 0.78039217,\n",
       "        0.76862746, 0.7882353 , 0.77254903, 0.78039217, 0.6862745 ,\n",
       "        0.59607846, 0.46666667, 0.5019608 , 0.69411767, 0.7529412 ,\n",
       "        0.7411765 , 0.7647059 , 0.77254903, 0.7607843 , 0.7764706 ,\n",
       "        0.78431374, 0.79607844, 0.79607844, 0.8117647 , 0.8156863 ,\n",
       "        0.8117647 , 0.7882353 , 0.78431374, 0.7921569 , 0.7882353 ,\n",
       "        0.7764706 , 0.67058825, 0.7176471 , 0.7372549 , 0.74509805,\n",
       "        0.7294118 , 0.7607843 , 0.6313726 , 0.6431373 , 0.63529414,\n",
       "        0.5137255 , 0.30588236, 0.4627451 , 0.67058825, 0.75686276,\n",
       "        0.7529412 , 0.76862746, 0.76862746, 0.75686276, 0.74509805,\n",
       "        0.78039217, 0.8       , 0.80784315, 0.8117647 , 0.8156863 ,\n",
       "        0.7921569 , 0.7882353 , 0.7882353 , 0.7882353 , 0.78039217,\n",
       "        0.78039217, 0.5686275 , 0.65882355, 0.6901961 , 0.6784314 ,\n",
       "        0.7372549 , 0.7647059 , 0.7921569 , 0.54901963, 0.4627451 ,\n",
       "        0.34901962, 0.22745098, 0.4745098 , 0.654902  , 0.73333335,\n",
       "        0.7607843 , 0.74509805, 0.7490196 , 0.7411765 , 0.7372549 ,\n",
       "        0.7647059 , 0.79607844, 0.7882353 , 0.7921569 , 0.8       ,\n",
       "        0.7921569 , 0.7882353 , 0.78039217, 0.7882353 , 0.79607844,\n",
       "        0.79607844, 0.6431373 , 0.5686275 , 0.6862745 , 0.7490196 ,\n",
       "        0.6745098 , 0.6745098 , 0.69411767, 0.70980394, 0.7254902 ,\n",
       "        0.7019608 , 0.2627451 , 0.46666667, 0.6431373 , 0.7529412 ,\n",
       "        0.7490196 , 0.7490196 , 0.73333335, 0.74509805, 0.7411765 ,\n",
       "        0.77254903, 0.8       , 0.8       , 0.80784315, 0.79607844,\n",
       "        0.7921569 , 0.78431374, 0.7764706 , 0.7882353 , 0.80784315,\n",
       "        0.78431374, 0.6862745 , 0.54901963, 0.6627451 , 0.7176471 ,\n",
       "        0.7490196 , 0.7411765 , 0.75686276, 0.76862746, 0.74509805,\n",
       "        0.7372549 , 0.63529414, 0.49411765, 0.6509804 , 0.73333335,\n",
       "        0.74509805, 0.7294118 , 0.7294118 , 0.7372549 , 0.7372549 ,\n",
       "        0.78039217, 0.7764706 , 0.8039216 , 0.8156863 , 0.81960785,\n",
       "        0.78431374, 0.7921569 , 0.7921569 , 0.78039217, 0.78039217,\n",
       "        0.7882353 , 0.7254902 , 0.5372549 , 0.63529414, 0.6666667 ,\n",
       "        0.6784314 , 0.6862745 , 0.74509805, 0.7607843 , 0.75686276,\n",
       "        0.74509805, 0.72156864, 0.5372549 , 0.68235296, 0.7411765 ,\n",
       "        0.7254902 , 0.7254902 , 0.73333335, 0.73333335, 0.7294118 ,\n",
       "        0.78431374, 0.79607844, 0.8039216 , 0.8156863 , 0.79607844,\n",
       "        0.78431374, 0.8       , 0.7921569 , 0.76862746, 0.77254903,\n",
       "        0.7921569 , 0.77254903, 0.61960787, 0.61960787, 0.6666667 ,\n",
       "        0.7294118 , 0.7254902 , 0.7254902 , 0.75686276, 0.7882353 ,\n",
       "        0.78431374, 0.7529412 , 0.6627451 , 0.19607843, 0.7254902 ,\n",
       "        0.72156864, 0.7254902 , 0.73333335, 0.7254902 , 0.72156864,\n",
       "        0.7764706 , 0.7882353 , 0.79607844, 0.8039216 , 0.79607844,\n",
       "        0.7921569 , 0.7882353 , 0.7882353 , 0.80784315, 0.7882353 ,\n",
       "        0.8039216 , 0.77254903, 0.69803923, 0.5411765 , 0.6901961 ,\n",
       "        0.77254903, 0.78431374, 0.73333335, 0.74509805, 0.79607844,\n",
       "        0.79607844, 0.7764706 , 0.2509804 , 0.15686275, 0.17254902,\n",
       "        0.70980394, 0.7137255 , 0.7176471 , 0.7176471 , 0.73333335,\n",
       "        0.77254903, 0.78431374, 0.79607844, 0.7764706 , 0.8117647 ,\n",
       "        0.8235294 , 0.7764706 , 0.8       , 0.7882353 , 0.79607844,\n",
       "        0.7764706 , 0.7764706 , 0.7372549 , 0.59607846, 0.57254905,\n",
       "        0.7529412 , 0.79607844, 0.79607844, 0.80784315, 0.78039217,\n",
       "        0.79607844, 0.69411767, 0.18431373, 0.17254902, 0.17254902,\n",
       "        0.1254902 , 0.7137255 , 0.70980394, 0.70980394, 0.72156864,\n",
       "        0.77254903, 0.8117647 , 0.8039216 , 0.7921569 , 0.79607844,\n",
       "        0.8156863 , 0.8       , 0.8       , 0.78431374, 0.77254903,\n",
       "        0.77254903, 0.75686276, 0.7647059 , 0.64705884, 0.43529412,\n",
       "        0.7058824 , 0.78431374, 0.8156863 , 0.81960785, 0.8117647 ,\n",
       "        0.8117647 , 0.20784314, 0.19607843, 0.18431373, 0.17254902,\n",
       "        0.15686275, 0.18039216, 0.18039216, 0.7137255 , 0.70980394,\n",
       "        0.7882353 , 0.78039217, 0.7882353 , 0.7921569 , 0.79607844,\n",
       "        0.78431374, 0.7882353 , 0.78431374, 0.78431374, 0.78431374,\n",
       "        0.77254903, 0.75686276, 0.75686276, 0.6901961 , 0.54901963,\n",
       "        0.6117647 , 0.73333335, 0.78039217, 0.8156863 , 0.8039216 ,\n",
       "        0.23137255, 0.20784314, 0.20392157, 0.2       , 0.20392157,\n",
       "        0.1882353 , 0.13333334, 0.1764706 , 0.19215687, 0.7019608 ,\n",
       "        0.7764706 , 0.8039216 , 0.7882353 , 0.8       , 0.8039216 ,\n",
       "        0.78039217, 0.79607844, 0.78431374, 0.78431374, 0.78039217,\n",
       "        0.7647059 , 0.7921569 , 0.75686276, 0.7294118 , 0.6039216 ,\n",
       "        0.43529412, 0.6627451 , 0.7529412 , 0.78039217, 0.24313726,\n",
       "        0.20784314, 0.21960784, 0.21960784, 0.12941177, 0.22745098,\n",
       "        0.22745098, 0.21960784, 0.10588235, 0.17254902, 0.6901961 ,\n",
       "        0.78431374, 0.78431374, 0.78431374, 0.79607844, 0.7921569 ,\n",
       "        0.76862746, 0.7921569 , 0.76862746, 0.76862746, 0.78431374,\n",
       "        0.7607843 , 0.7607843 , 0.7529412 , 0.75686276, 0.6745098 ,\n",
       "        0.5294118 , 0.5647059 , 0.7176471 , 0.2509804 , 0.23529412,\n",
       "        0.23529412, 0.22745098, 0.16078432, 0.2509804 , 0.23137255,\n",
       "        0.20392157, 0.20392157, 0.20392157, 0.10196079, 0.6862745 ,\n",
       "        0.78039217, 0.78431374, 0.78039217, 0.7764706 , 0.76862746,\n",
       "        0.7882353 , 0.76862746, 0.78039217, 0.78039217, 0.75686276,\n",
       "        0.75686276, 0.76862746, 0.7529412 , 0.74509805, 0.7019608 ,\n",
       "        0.58431375, 0.14117648, 0.27058825, 0.23137255, 0.20392157,\n",
       "        0.21960784, 0.14901961, 0.23529412, 0.21960784, 0.21960784,\n",
       "        0.20392157, 0.2509804 , 0.09019608, 0.19607843, 0.1882353 ,\n",
       "        0.77254903, 0.78431374, 0.77254903, 0.7764706 , 0.7607843 ,\n",
       "        0.78039217, 0.7764706 , 0.7921569 , 0.76862746, 0.7490196 ,\n",
       "        0.7372549 , 0.75686276, 0.74509805, 0.7411765 , 0.73333335,\n",
       "        0.22352941, 0.23137255, 0.23137255, 0.22745098, 0.22352941,\n",
       "        0.1254902 , 0.23921569, 0.19215687, 0.24705882, 0.21568628,\n",
       "        0.24313726, 0.10588235, 0.19607843, 0.19215687, 0.20392157,\n",
       "        0.7647059 , 0.78039217, 0.78039217, 0.78431374, 0.78039217,\n",
       "        0.7882353 , 0.7764706 , 0.78039217, 0.76862746, 0.7607843 ,\n",
       "        0.7607843 , 0.7529412 , 0.7372549 , 0.7294118 , 0.23921569,\n",
       "        0.25490198, 0.23921569, 0.23137255, 0.22745098, 0.12156863,\n",
       "        0.21960784, 0.15686275, 0.2627451 , 0.23921569, 0.22352941,\n",
       "        0.15294118, 0.20784314, 0.20784314, 0.21568628, 0.22745098,\n",
       "        0.77254903, 0.78039217, 0.7764706 , 0.78039217, 0.7764706 ,\n",
       "        0.7882353 , 0.78039217, 0.7647059 , 0.7607843 , 0.7647059 ,\n",
       "        0.74509805, 0.7490196 , 0.7490196 , 0.41568628, 0.22745098,\n",
       "        0.24313726, 0.23921569, 0.21960784, 0.16470589, 0.1764706 ,\n",
       "        0.17254902, 0.26666668, 0.25490198, 0.22352941, 0.20784314,\n",
       "        0.22745098, 0.22745098, 0.21960784, 0.22352941, 0.23137255,\n",
       "        0.78039217, 0.78039217, 0.78431374, 0.78431374, 0.78039217,\n",
       "        0.76862746, 0.76862746, 0.76862746, 0.7607843 , 0.7607843 ,\n",
       "        0.75686276, 0.7490196 , 0.69411767, 0.23529412, 0.23529412,\n",
       "        0.22352941, 0.21960784, 0.16470589, 0.14509805, 0.1882353 ,\n",
       "        0.2784314 , 0.2509804 , 0.1764706 , 0.22352941, 0.23529412,\n",
       "        0.23137255, 0.21960784, 0.21176471, 0.24705882, 0.23137255]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(cat)\n",
    "predictions\n",
    "# print('Predicted:', decode_predictions(predictions, top=3)[0])\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
